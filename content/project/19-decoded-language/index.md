---
slug: decoded-language
title: "Decoded Language"
subtitle: ""
summary: "Interactive sound installation for Supercollider and Kinect V2. IEM, 2019."
authors: [admin]
tags: [installation, sound installation, interactive installation, Supercollider, media art, TD]
# Add videos
categories: [projects]
date: 2019-01-24T18:18:58+03:00
#lastmod: 2023-08-06T18:18:58+03:00
featured: false
draft: false

# Featured image
image:
  caption: "3D rendering of audience performance"
  focal_point: "Left"
  preview_only: false

links:
- name: 'Installation flyer'
  url: '19-cmiltiadis-decoded-language.pdf'
  icon: 'file-pdf'
  icon_pack: fas

# Projects (optional).
projects: []
---

***Decoded language*** is a multi-user interactive sound installation for Supercollider and Kinect V2.
The installation uses real-time body (skeleton) tracking as interface for controlling a virtual granular synthesis instrument. 
This way, the installation produces an entanglement of the spatial domain of bodily movement with the temporal domain of sound.
Therefore, the installation invites the audience to create spatiotemporal choreographies and experiece how spatial movement correlates to temporal manipulations of the originally linear dimension of listening.   
The interaction interface was designed to be intrinsic and relative to one's body rather than absolute. Therefore absolute positions are irrelevant, only body configurations, relative displacements and rotations matter. 
Finally, all interaction data are recorded and are visualized geometrically at the end of each session, offering a visual-spatial representation of the temporal manipulations of the listening timeline (see [images below](#renderings)).   
The installation uses audio sampled from Saul Williams' spoken word performance of [*coded language*](https://www.youtube.com/watch?v=jzY2-GRDiPM) at Def Poetry Jam (2004).


<!-- 

The audience is invited to create their own choreographies and temporal articulations in space through bodily . After each interactive session, the audible temporal manipulations of the user[s]’ performance are rendered to a digital geometry.
Using the body as control interface the installation offers a spatial interface to  


The installation records 
+ fine control 
+ rendering end 
The installation “**decoded language**” uses granular synthesis as a means of folding a given sound, and a transposition from the time domain to the space domain, through a play of manipulation operators that introduce curvature to the original dimension of listening time. 

It employs real-time body tracking as a spatial engagement interface, which the audience can excite and manipulate while exploring the resolution of granular folding. 

The space of interaction is treated as intrinsic to the body instead of absolute, where positions are irrelevant, and only configurations, displacements and orientations of the body matter. 
The audience, through their body, is invited to create their own choreographies of temporal articulations in space. After each interactive session, the audible temporal manipulations of the user[s]’ performance are rendered to a digital geometry.
-->


# Project information 

**decoded language** was developed between late-2018 and early-2019 at the Institute of Electronic Music (IEM), University of Music and Performing Arts, Graz, with the support of Prof. Marko Ciciliani and Prof. Daniel Mayer. 
<!-- 
is a multi-user interactive sound installation for Supercollider and Kinect V2. 
-->

The installation consists of an algorithmic synthesizer developed in [Supercollider](https://supercollider.github.io/), and a 3D tracking and visualization software written in [Processing](https://processing.org/), synchronized with each other via OSC.  

The work was first presented at the IEM CUBE Concert on January 24, 2019.  
A previous version of the installation, developed with [Pure Data](https://puredata.info/) and Kinect V1, was presented in December 2017 at the Institute of Architecture and Media, TU Graz.


<!-- 
Multi-user interactive sound installation for Supercollider, Java, Kinect V2 and projector. 
Institute of Electronic Music, University of Music and Performing Arts, Graz, 2019
-->

# 3D visualizations of audience performances {#renderings}

{{< gallery album="decoded-language">}}
